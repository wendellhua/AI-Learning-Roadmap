# 💻 Local Deployment | 本地部署与量化

> 🎯 **学习目标**：学会在自己的电脑上运行AI大模型

---

## 🌟 为什么要本地部署？

在自己电脑上运行AI的好处：
- 🔒 **隐私安全** - 数据不离开你的电脑
- 💰 **免费使用** - 不用付API费用
- ⚡ **离线可用** - 没网也能用
- 🎮 **自由定制** - 想怎么玩就怎么玩

---

## 📚 本章内容

### 1️⃣ [Ollama GPU加速 | Ollama GPU Acceleration](01_Ollama_GPU_Acceleration.md)

最简单的本地部署方案：
- 📥 **一键安装** - 5分钟搞定
- ⚡ **GPU加速** - 让模型飞起来
- 🔧 **常用命令** - 快速上手指南

### 2️⃣ [模型量化指南 | Model Quantization Guide](02_Model_Quantization_Guide.md)

让大模型"瘦身"的魔法：
- 📦 **什么是量化** - 压缩模型的技术
- 🎯 **量化等级** - Q4、Q5、Q8的区别
- ⚖️ **精度vs速度** - 如何平衡

### 3️⃣ [本地模型评估 | Local Model Evaluation](03_Local_Model_Evaluation.md)

如何判断模型好不好：
- 📊 **评估指标** - 衡量模型能力
- 🧪 **测试方法** - 实际测试技巧
- 📈 **性能对比** - 各模型实测数据

---

## 🎮 快速入门指南

```bash
# 1. 安装 Ollama（以Mac为例）
brew install ollama

# 2. 启动服务
ollama serve

# 3. 下载并运行模型
ollama run llama3.1:8b

# 4. 开始对话！
>>> 你好，请介绍一下你自己
```

---

## 📊 显存需求速查表

| 模型大小 | 量化等级 | 所需显存 | 推荐显卡 |
|---------|---------|---------|---------|
| 7B | Q4 | 4-6 GB | RTX 3060 |
| 7B | Q8 | 8-10 GB | RTX 3080 |
| 13B | Q4 | 8-10 GB | RTX 3080 |
| 70B | Q4 | 35-40 GB | RTX 4090 x2 |

---

## ⏱️ 预计学习时间

- Ollama安装使用：1-2 小时
- 量化原理学习：1-2 小时
- 模型评估实践：1-2 小时

**总计**：约 3-6 小时

---

> 💡 **小贴士**：从 Ollama 开始是最简单的！几分钟就能在自己电脑上运行AI。
